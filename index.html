<!DOCTYPE html>
<html>
  <head>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/uikit@3.3.1/dist/css/uikit.min.css" /> 
    <link rel="stylesheet" href="mystyle.css">
    <script src="https://cdn.jsdelivr.net/npm/uikit@3.3.1/dist/js/uikit.min.js"></script> 
    <script src="https://cdn.jsdelivr.net/npm/uikit@3.18.2/dist/js/uikit-icons.min.js"></script> 
    <script src="js/uikit-icons.js"></script>
    <script src="js/to-top.js"></script>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="author" content="Shiu-hong CHEN">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Shiu-hong Kao</title>
    <link rel = "icon" href = "src/icon.png"
        type = "image/x-icon">
  </head>
  <body style="background-color: aliceblue;">
    <!-- Button -->
    <button id="scrollToTopBtn" onclick="scrollToTop()">&#9650;</button>
    <script src="./js/to-top.js"></script>
    <div uk-sticky="sel-target: .uk-navbar-container; cls-active: uk-navbar-sticky; end: !.uk-height-large;" style="padding-bottom:20px;">
        <nav class="uk-navbar-container uk-margin uk-light" style="background-color: #7e9fde" uk-navbar>
            <div class="uk-navbar-center">
                <span class="uk-navbar-item uk-logo uk-text-primary" href="index.html" style="font-size: 24px; font-family:'Lato', Verdana, Helvetica, sans-serif; text-transform: capitalize;">Shiu-hong Kao (高旭宏)</span><br/>
                <ul class="uk-navbar-nav">
                    <li class="uk-active uk-visible@m"> <a href="#Research"><span class="navi" style="font-size: 20px;"><i>Research</i></span></a> </li>
                    <li class="uk-active uk-visible@m"> <a href="#Experience"><span class="navi" style="font-size: 20px;"><i>Experience</i></span></a> </li>
                    <li class="uk-active uk-visible@m"> <a href="#Awards"><span class="navi" style="font-size: 20px;"><i>Awards</i></span></a> </li>
                    <a class="uk-active uk-navbar-toggle uk-hidden@m" uk-navbar-toggle-icon href="#offcanvas-nav-primary" uk-toggle></a>
                </ul>
            </div>
        </nav>
    </div>
    <div id="offcanvas-nav-primary" uk-offcanvas="overlay: true">
        <div class="uk-offcanvas-bar uk-flex uk-flex-column">
            <ul class="uk-nav uk-nav-primary uk-nav-center uk-margin-auto-vertical">
                <li class="uk-active"> <a href="#Research"><span class="navi">Research</span></a> </li>
                <li class="uk-active"> <a href="#Experience"><span class="navi">Experience</span></a> </li>
                <li class="uk-active"> <a href="#Awards"><span class="navi">Awards</span></a> </li>
            </ul>
        </div>
    </div>
    <center>
    <div style="width: 100%; background-color:aliceblue">
        <div id="bio" style="text-align: left; width:100%; max-width: 1100px; font-size: small; padding-bottom:40px">
            <table style="width:90%; margin-left: 5%; margin-right: 5%; max-width: 1100px; text-align: center;">
                <tbody>
                    <tr>
                        <td class="bio_left" style="text-align: left;width:40%;max-width:500px;padding-left: 15px;">
                            <center>
                            <img src="src/profile.jpg" class="avatar" style="width: 100%; max-width: 180px;"/><br/>
                            </center>
                                
                            
                        </td>
                        <td style="text-align: left;width:60%;padding-left: 15px;">
                            <div style="color: gray; font-size: small;"><span uk-icon="icon: user"></span> Alt name: Daniel Kao, Xuhong Kao</div>
                            <a href = "https://scholar.google.com/citations?user=hpD2pbwAAAAJ&hl=en&oi=ao" target="_blank"><span style="color: gray; width: 20px; height:20px;" uk-icon="icon: google-scholar"></span><span style="color: gray; font-size: small;"> Google Scholar</span></a><br/>
                            <a style="color: gray; font-size: small;" href = "https://www.linkedin.com/in/danielshkao/" target="_blank"><span uk-icon="icon: linkedin"></span><span style="color: gray; font-size: small;"> LinkedIn</span></a><br/>
                            <a style="color: gray; font-size: small;" href = "https://github.com/DanielSHKao" target="_blank"><span uk-icon="icon: github"></span><span style="color: gray; font-size: small;"> Github</span></a><br/>
                            <a style="color: gray; font-size: small;" href = "https://dblp.org/pid/342/2799" target="_blank"><span style="width: 20px; height:20px;" uk-icon="icon: dblp"></span><span style="color: gray; font-size: small;"> DBLP</span></a><br/>
                            <a href = "https://x.com/danielshkao" target="_blank"><span style="color: gray; width: 20px; height:20px;" uk-icon="icon: x"></span><span style="color: gray; font-size: small;"> X (Twitter)</span></a><br/>
                            <div class="button-row">
        
                                <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#115;&#104;&#107;&#97;&#111;&#64;&#117;&#46;&#110;&#117;&#115;&#46;&#101;&#100;&#117;" class="btn btn-clay" >
                                Contact me via Email
                                </a>
                            
                                <a href="src/shkao_CV.pdf" class="btn btn-clay" download>
                                Download my CV
                                </a>
                            
                            </div>
                        </td>
                    </tr>
                </tbody>
            </table>
            <div style="width:90%; margin-left: 5%; margin-right: 5%; font-size: 14px">
                <p>
                    I am a PhD student in <a href="https://www.comp.nus.edu.sg/programmes/pg/phdcs/" target="_blank">Computer Science</a> at National University of Singapore 
                    (<a href="https://nus.edu.sg/" target="_blank">NUS</a>). 
                    My research interests sit at the intersection of deep learning and computer vision, with a specific focus on <b>diffusion models</b>, <b>multimodal large language models</b>, and <b>3D reconstruction</b>.
                    I am particularly driven by the goal of leveraging the potential of pre-trained models in vision-language challenges through modular design and efficient adaptation techniques.
                </p>    
                <p>
                    Previously, I received the <a href="https://cse.hkust.edu.hk/pg/" target="_blank">M.Phil.</a> in Computer Science and Engineering from the Hong Kong University of Science and Technology (<a href="https://hkust.edu.hk/" target="_blank">HKUST</a>) under the supervision of
                    Prof. <a href="http://www.cs.ust.hk/~cktang" target="_blank">Chi-Keung Tang</a>, collaborating closely with Prof. <a href="https://yuwingtai.github.io/" target="_blank">Yu-Wing Tai</a> from <a href="https://home.dartmouth.edu/" target="_blank">Dartmouth College</a>. 
                    I also served as a research intern in the Media Computing Group at <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/" target="_blank">Microsoft Research Asia</a>, working with Dr. <a href="https://pableeto.github.io/">Xiao Li</a> and Dr. <a href="https://jingluw.github.io/" target="_blank">Jinglu Wang</a>.
                    Prior to my graduate studies, I earned the Bachelor of Science degree from HKUST, double-majoring in <a href="https://dsct.hkust.edu.hk/" target="_blank">Data Science and Technology</a> and in <a href="https://cse.hkust.edu.hk/bsc/" target="_blank">Computer Science</a>. 
                    I completed my undergraduate thesis in Prof. <a href="https://www.cse.ust.hk/~gchan/" target="_blank">Shueng-Han Gary Chan</a>'s lab, in cooperation with Dr. <a href="https://jierunchen.github.io/" target="_blank">Jierun Chen</a>.
                </p>
                <div style="margin-bottom: 20px;">
                    <b style="font-size: 16px;">- News:</b><br/>
                    <table style="width:100%; border-collapse: collapse; line-height: 1.6;">
                        <tr>
                            <td style="vertical-align: top; padding: 4px 0;"><strong>Jan. 2026</strong></td>
                            <td style="padding: 4px 0;"><a href="https://danielshkao.github.io/cot-rvs.html" target="_blank" style="color: #646464; text-decoration: underline;">CoT-RVS</a> is accepted to <strong>ICLR 2026</strong>.</td>
                        </tr>
                        <tr>
                          <td style="width: 20%; vertical-align: top; padding: 4px 0;"><strong>Jan. 2026</strong></td>
                          <td style="padding: 4px 0;">I joined the PhD program at <strong>NUS</strong>.</td>
                        </tr>
                        <tr>
                            <td style="width: 20%; vertical-align: top; padding: 4px 0;"><strong>Nov. 2025</strong></td>
                            <td style="padding: 4px 0;">A new preprint is released: <a href="./cot-seg.html" target="_blank" style="color: #646464; text-decoration: underline;">CoT-Seg</a>.</td>
                        </tr>
                        <tr>
                          <td style="vertical-align: top; padding: 4px 0;"><strong>Jun. 2025</strong></td>
                          <td style="padding: 4px 0;"><a href="https://arxiv.org/abs/2503.06235" target="_blank" style="color: #646464; text-decoration: underline;">StreamGS</a> is accepted to <strong>ICCV 2025</strong>.</td>
                        </tr>
                        <tr>
                          <td style="vertical-align: top; padding: 4px 0;"><strong>Jan. 2025</strong></td>
                          <td style="padding: 4px 0;">I was honored to receive the <strong>Stars of Tomorrow Award</strong> from Microsoft Research Asia.</td>
                        </tr>
                      </table>
                        <p style="font-size: small;"><b>- About my name:</b> I go by the name <i>Daniel</i> in daily life. My Chinese name, Shiu-hong (pronounced similarly to <i>"she hong"</i> with an <i>"o"</i> sound), is typically used only in official documents and my publications.
                            In mainland China, I am sometimes referred to as <i>Xuhong</i>, which is based on a different homophonic transliteration system.
                        </p>
                </div>
            </div>
            
        </div>
    </div>
    <div style="width: 100%; background-color:white">
        <div id="Research" style="text-align: left; width: 100%; max-width: 1100px; padding-bottom:40px; padding-top:40px;" class="anchor-target">
            <div style="width:90%; margin-left: 5%; margin-right: 5%;">
            <h3 style="font-size:23px; font-family: 'Lato', Verdana, Helvetica, sans-serif; text-transform: capitalize; padding-left: 20pt;text-decoration: underline;">Publications</h3>
                <table id="pub_table" style="width:100%;">
                    <tr>
                        <td class="pub_left desktop"><center>
                            <div class="one" onmouseout="ThinkVideo_stop()" onmouseover="ThinkVideo_start()">
                            <div class="two" id='ThinkVideo_image'><video  class="pub_video" muted autoplay loop>
                            <source src="src/thinkvideo-small.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                            </video></div>
                            <img src='src/thinkvideo.png'  class="pub_img">
                        </div></center>
                            <script type="text/javascript">
                            function ThinkVideo_start() {
                                document.getElementById('ThinkVideo_image').style.opacity = "1";
                            }
                
                            function ThinkVideo_stop() {
                                document.getElementById('ThinkVideo_image').style.opacity = "0";
                            }
                            ThinkVideo_stop()
                            </script>
                        </td>
                        <td class="pub_right">
                            <div class="one phone" onmouseout="ThinkVideo_video_stop()" onmouseover="ThinkVideo_video_start()">
                                <div><center>
                                    <div class="two" id='ThinkVideo_video_image'><video  class="pub_video_phone" muted autoplay loop>
                                    <source src="src/thinkvideo-small.mp4" type="video/mp4">
                                    Your browser does not support the video tag.
                                    </video></div>
                                    <img src='src/thinkvideo.png'  class="pub_img_phone"></center>
                                </div>
                                </div>
                                <script type="text/javascript">
                                function ThinkVideo_video_start() {
                                    document.getElementById('ThinkVideo_video_image').style.opacity = "1";
                                }
                    
                                function ThinkVideo_video_stop() {
                                    document.getElementById('ThinkVideo_video_image').style.opacity = "0";
                                }
                                ThinkVideo_video_stop()
                            </script>
                            <b style="font-size: medium;">CoT-RVS: Zero-Shot Chain-of-Thought Reasoning Segmentation for Videos</b><br/>
                            <u>Shiu-hong Kao</u>, Yu-Wing Tai, Chi-Keung Tang<br/>
                            <i>International Conference on Learning Representations (ICLR), 2026</i><br/>
                            [<a href="https://arxiv.org/abs/2505.18561" target="_blank">arXiv</a>]
                            [<a href="https://danielshkao.github.io/cot-rvs.html" target="_blank">Project page</a>]<br/>
                            <p>We propose CoT-RVS to extract the temporal-semantic correlation in videos with chain of thoughts and achieve the state-of-the-art performance for reasoning video segmentation.</p>
                        </td>
                    </tr>
                    <tr>
                        <td class="pub_left desktop">
                            <div class='one'><center><img class="pub_img" src="src/cot-seg.jpg" /></center></div>
                        </td>
                        <td class="pub_right">
                            <div class='one phone'><center><img class="pub_img_phone" src="src/cot-seg.jpg"/></center></div>
                            <b style="font-size: medium;">CoT-Seg: Rethinking Segmentation with Chain-of-Thought Reasoning and Self-Correction</b><br/>
                            <u>Shiu-hong Kao*</u>, Chak-Ho Huang*, Huaiqian Liu*, Yu-Wing Tai, Chi-Keung Tang<br/> 
                            <i>Arxiv preprint 2026.</i><br/>
                            [<a href="http://arxiv.org/abs/2601.17420" target="_blank">arXiv</a>] [<a href="https://danielshkao.github.io/cot-seg.html" target="_blank">Project page</a>]
                            <p>
                                <i>*Equal contribution.</i> We propose CoT-Seg, a modular reasoning segmentation framework that significantly improves the quality of segmentation masks with chain-of-thought reasoning and self-correction loop.
                            </p>
                        </td>
                    </tr>
                    <tr>
                        <td class="pub_left desktop">
                            <div class='one'><center><img class="pub_img" src="src/thinkfirst.png" /></center></div>
                        </td>
                        <td class="pub_right">
                            <div class='one phone'><center><img class="pub_img_phone" src="src/thinkfirst.png"/></center></div>
                            <b style="font-size: medium;">Think Before You Segment: High-Quality Reasoning Segmentation with GPT Chain of Thoughts</b><br/>
                            <u>Shiu-hong Kao</u>, Yu-Wing Tai, Chi-Keung Tang<br/> 
                            <i>Arxiv preprint 2025.</i><br/>
                            [<a href="http://arxiv.org/abs/2503.07503" target="_blank">arXiv</a>] [<a href="https://danielshkao.github.io/thinkfirst.html" target="_blank">Project page</a>]
                            <p>
                                We propose ThinkFirst, a novel Chain-of-Thought (CoT) reasoning segmentation framework that generates an accurate object mask given a text prompt.
                                ThinkFirst can handle difficult scenarios such as implicit queries, camouflaged objects, out-of-domain objects with easy control.
                            </p>
                        </td>
                    </tr>
                    <tr>
                        <td class="pub_left desktop"><center>
                        <div class="one" onmouseout="UVRM_stop()" onmouseover="UVRM_start()">
                            <div class="two" id='UVRM_image'><video  class="pub_video" muted autoplay loop>
                            <source src="src/uvrm.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                            </video></div>
                            <img src='src/uvrm_teaser.jpg'  class="pub_img">
                        </div></center>
                        <script type="text/javascript">
                            function UVRM_start() {
                            document.getElementById('UVRM_image').style.opacity = "1";
                            }
                
                            function UVRM_stop() {
                            document.getElementById('UVRM_image').style.opacity = "0";
                            }
                            UVRM_stop()
                        </script>
                        </td>
                        <td class="pub_right">
                            <div class="one phone" onmouseout="UVRM_video_stop()" onmouseover="UVRM_video_start()">
                                <div><center>
                                    <div class="two" id='UVRM_video_image'><video  class="pub_video_phone" muted autoplay loop>
                                    <source src="src/uvrm.mp4" type="video/mp4">
                                    Your browser does not support the video tag.
                                    </video></div>
                                    <img src='src/uvrm_teaser.jpg'  class="pub_img_phone"></center>
                                </div>
                            </div>
                            <script type="text/javascript">
                                function UVRM_video_start() {
                                document.getElementById('UVRM_video_image').style.opacity = "1";
                                }
                    
                                function UVRM_video_stop() {
                                document.getElementById('UVRM_video_image').style.opacity = "0";
                                }
                                UVRM_video_stop()
                            </script>
                            <b style="font-size: medium;">UVRM: A Scalable 3D Reconstruction Model from Unposed Videos</b><br/>
                            <u>Shiu-hong Kao</u>, Xiao Li, Jinglu Wang, Yang Li, Chi-Keung Tang, Yu-Wing Tai, Yan Lu<br/>
                            <i>Arxiv preprint 2025.</i><br/>
                            [<a href="http://arxiv.org/abs/2501.09347" target="_blank">arXiv</a>] [<a href="https://danielshkao.github.io/src/uvrm_demo.mp4" target="_blank">Demo</a>]
                            <p>
                                We introduce UVRM, a novel 3D reconstruction model capable of being trained and evaluated on 360-degree monocular videos without requiring any information about the pose.
                            </p>
                        </td>
                    </tr>
                    <tr>
                        <td class="pub_left desktop"><center>
                            <div class="one" onmouseout="StreamGS_stop()" onmouseover="StreamGS_start()">
                            <div class="two" id='streamgs_image'><video  class="pub_video" muted autoplay loop>
                            <source src="src/streamgs.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                            </video></div>
                            <img src='src/streamgs_teaser.png'  class="pub_img">
                        </div></center>
                            <script type="text/javascript">
                            function StreamGS_start() {
                                document.getElementById('streamgs_image').style.opacity = "1";
                            }
                
                            function StreamGS_stop() {
                                document.getElementById('streamgs_image').style.opacity = "0";
                            }
                            StreamGS_stop()
                            </script>
                        </td>
                        <td class="pub_right">
                            <div class="one phone" onmouseout="StreamGS_video_stop()" onmouseover="StreamGS_video_start()">
                                <div><center>
                                    <div class="two" id='StreamGS_video_image'><video  class="pub_video_phone" muted autoplay loop>
                                    <source src="src/streamgs.mp4" type="video/mp4">
                                    Your browser does not support the video tag.
                                    </video></div>
                                    <img src='src/streamgs_teaser.png'  class="pub_img_phone"></center>
                                </div>
                                </div>
                                <script type="text/javascript">
                                function StreamGS_video_start() {
                                    document.getElementById('StreamGS_video_image').style.opacity = "1";
                                }
                    
                                function StreamGS_video_stop() {
                                    document.getElementById('StreamGS_video_image').style.opacity = "0";
                                }
                                StreamGS_video_stop()
                                </script>
                                <b style="font-size: medium;">StreamGS: Online Generalizable Gaussian Splatting Reconstruction for Unposed Image Streams</b><br/>
                                Yang Li, Jinglu Wang, Lei Chu, Xiao Li, <u>Shiu-hong Kao</u>, Yingcong Chen, Yan Lu<br/> 
                                <i>International Conference on Computer Vision (ICCV), 2025</i><br/>
                                [<a href="https://arxiv.org/abs/2503.06235" target="_blank">arXiv</a>]
                                <p>
                                    We propose StreamGS, an online generalizable 3DGS reconstruction method for unposed image streams, 
                                    which progressively transform image streams to 3D Gaussian streams by predicting and aggregating per-frame Gaussians.
                                </p>
                        </td>
                    </tr>
                    <tr>
                        <td class="pub_left desktop">
                            <div class='one'><center><img class="pub_img" src="src/igct.png" /></center></div>
                        </td>
                        <td class="pub_right">
                            <div class='one phone'><center><img class="pub_img_phone" src="src/igct.png"/></center></div>
                            <b style="font-size: medium;">Beyond and Free from Diffusion: Invertible Guided Consistency Training</b><br/>
                            Chia-Hong Hsu, <u>Shiu-hong Kao</u>, Randall Balestriero<br/> 
                            <i>Arxiv preprint 2025.</i><br/>
                            [<a href="https://arxiv.org/abs/2502.05391" target="_blank">arXiv</a>] [<a href="https://swimmincatt35.github.io/iGCT/" target="_blank">Project page</a>]
                            <p>
                                We propose invertible Guided Consistency Training (iGCT), a data-driven training framework for guided consistency model, 
                                contributing to fast and guided image generation and editing without requiring the training and distillation of any diffusion models.
                            </p>
                        </td>
                    </tr>
                    <tr>
                        <td class="pub_left desktop"><center>
                        <div class="one" onmouseout="DeceptiveHuman_stop()" onmouseover="DeceptiveHuman_start()">
                            <div class="two" id='DeceptiveHuman_image'><video  class="pub_video" muted autoplay loop>
                            <source src="src/deceptivehuman.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                            </video></div>
                            <img src='src/deceptivehuman.png'  class="pub_img">
                        </div></center>
                        <script type="text/javascript">
                            function DeceptiveHuman_start() {
                            document.getElementById('DeceptiveHuman_image').style.opacity = "1";
                            }
                
                            function DeceptiveHuman_stop() {
                            document.getElementById('DeceptiveHuman_image').style.opacity = "0";
                            }
                            DeceptiveHuman_stop()
                        </script>
                        </td>
                        <td class="pub_right">
                            <div class="one phone" onmouseout="DeceptiveHuman_video_stop()" onmouseover="DeceptiveHuman_video_start()">
                                <div><center>
                                    <div class="two" id='DeceptiveHuman_video_image'><video  class="pub_video_phone" muted autoplay loop>
                                    <source src="src/deceptivehuman.mp4" type="video/mp4">
                                    Your browser does not support the video tag.
                                    </video></div>
                                    <img src='src/deceptivehuman.png'  class="pub_img_phone"></center>
                                </div>
                            </div>
                            <script type="text/javascript">
                                function DeceptiveHuman_video_start() {
                                document.getElementById('DeceptiveHuman_video_image').style.opacity = "1";
                                }
                    
                                function DeceptiveHuman_video_stop() {
                                document.getElementById('DeceptiveHuman_video_image').style.opacity = "0";
                                }
                                DeceptiveHuman_video_stop()
                            </script>
                            <b style="font-size: medium;">InceptionHuman: Controllable Prompt-to-NeRF for Photorealistic 3D Human Generation</b><br/>
                            <u>Shiu-hong Kao</u>, Xinhang Liu, Yu-Wing Tai, Chi-Keung Tang<br/>
                            <i>Arxiv preprint 2024.</i><br/>
                            [<a href="https://arxiv.org/abs/2311.16499" target="_blank">arXiv</a>]
                            <p>
                                We propose InceptionHuman, a NeRF-based generative framework incorporating state-of-the-art diffusion models, which receives any types and any sizes of prompts, (<i>e.g. text, pose, style</i>) to generate realistic 3D human.
                            </p>
                        </td>
                    </tr>
                    <tr>
                        <td class="pub_left desktop"><center>
                            <div class="one" onmouseout="DeceptiveNeRF_stop()" onmouseover="DeceptiveNeRF_start()">
                            <div class="two" id='DeceptiveNeRF_image'><video  class="pub_video" muted autoplay loop>
                            <source src="src/deceptivenerf.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                            </video></div>
                            <img src='src/deceptivenerf.png'  class="pub_img">
                        </div></center>
                            <script type="text/javascript">
                            function DeceptiveNeRF_start() {
                                document.getElementById('DeceptiveNeRF_image').style.opacity = "1";
                            }
                
                            function DeceptiveNeRF_stop() {
                                document.getElementById('DeceptiveNeRF_image').style.opacity = "0";
                            }
                            DeceptiveNeRF_stop()
                            </script>
                        </td>
                        <td class="pub_right">
                            <div class="one phone" onmouseout="DeceptiveNeRF_video_stop()" onmouseover="DeceptiveNeRF_video_start()">
                                <div><center>
                                    <div class="two" id='DeceptiveNeRF_video_image'><video  class="pub_video_phone" muted autoplay loop>
                                    <source src="src/deceptivenerf.mp4" type="video/mp4">
                                    Your browser does not support the video tag.
                                    </video></div>
                                    <img src='src/deceptivenerf.png'  class="pub_img_phone"></center>
                                </div>
                                </div>
                                <script type="text/javascript">
                                function DeceptiveNeRF_video_start() {
                                    document.getElementById('DeceptiveNeRF_video_image').style.opacity = "1";
                                }
                    
                                function DeceptiveNeRF_video_stop() {
                                    document.getElementById('DeceptiveNeRF_video_image').style.opacity = "0";
                                }
                                DeceptiveNeRF_video_stop()
                            </script>
                            <b style="font-size: medium;">Diffusion-Generated Pseudo-Observations for High-Quality Sparse-View Reconstruction</b><br/>
                            Xinhang Liu, Jiaben Chen, <u>Shiu-hong Kao</u>, Yu-Wing Tai, Chi-Keung Tang<br/>
                            <i>European Conference on Computer Vision (ECCV), 2024</i><br/>
                            [<a href="https://arxiv.org/abs/2305.15171" target="_blank">arXiv</a>]
                            [<a href="https://deceptive-nerf.github.io/" target="_blank">Project page</a>]<br/>
                            <p>We introduce Deceptive-NeRF/3DGS, a new method for enhancing the quality of reconstructed NeRF/3DGS models using synthetically generated pseudo-observations, capable of handling sparse input and removing floater artifacts.</p>
                        </td>
                    </tr>
                    <tr>
                        <td class="pub_left desktop">
                            <div class='one'><center><img class="pub_img" src="src/StableKD.png" /></center></div>
                        </td>
                        <td class="pub_right">
                            <div class='one phone'><center><img class="pub_img_phone" src="src/StableKD.png"/></center></div>
                            <b style="font-size: medium;">StableKD: Breaking Inter-block Optimization Entanglement for Stable Knowledge Distillation</b><br/>
                            <u>Shiu-hong Kao</u>*, Jierun Chen*, S.-H. Gary Chan<br/> 
                            <i>Arxiv preprint 2023.</i><br/>
                            [<a href="https://arxiv.org/abs/2312.13223" target="_blank">arXiv</a>]
                            [<a href="https://github.com/DanielSHKao/StableKD" target="_blank">Code</a>]<br/>
                            <p>
                                <i>*Equal contribution.</i> We reveal the issue of Inter-block Optimization Entanglement (IBOE) in 
                                end-to-end KD training and further propose StableKD to stablilize optimization.
                                Extensive experiments show StableKD achieve high accuracy, fast convergence, and high data efficiency.

                            </p>
                        </td>
                    </tr>
                    <tr>
                        <td class="pub_left desktop">
                            <div class='one'><center><img class="pub_img" src="src/pconv.png"/></center></div>
                        </td>
                        <td class="pub_right ">
                            <div class='one phone'><center><img class="pub_img_phone" src="src/pconv.png"/></center></div>
                            <b style="font-size: medium;">Run, Don't Walk: Chasing Higher FLOPS for Faster Neural Networks</b><br/>
                            Jierun Chen, <u>Shiu-hong Kao</u>, Hao He, Weipeng Zhuo, Song Wen, Chul-Ho Lee, S.-H. Gary Chan<br/>
                            <i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023</i><br/>
                            [<a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_Run_Dont_Walk_Chasing_Higher_FLOPS_for_Faster_Neural_Networks_CVPR_2023_paper.html" target="_blank">Paper</a>]
                            [<a href="https://github.com/JierunChen/FasterNet" target="_blank">Code</a>]
                            <iframe src="https://ghbtns.com/github-btn.html?user=JierunChen&repo=FasterNet&type=star&count=true&size=small" frameborder="0" scrolling="0" width="100" height="20" title="GitHub"></iframe>	
                            <br/>
                            <p>We propose a simple yet fast and effective partial convolution (PConv), as well as a latency-efficient family of network architectures called FasterNet.</p>
                        </td>
                    </tr>
                </table>
                <!--<h3 style="font-size:23px; font-family: 'Lato', Verdana, Helvetica, sans-serif; text-transform: capitalize; padding-left: 20pt;text-decoration: underline;" >Projects</h3>
                <ul style="text-align: left;">
                    <li>
                        <b style="font-size: medium;">Dynamic Neural Network Comparison: Efficiency and Performance</b><br/>
                        <u>Shiu-hong Kao</u>, Jierun Chen, S.-H. Gary Chan<br/>
                        <i>Undergraduate Research Opportunities Program, Fall 2021, HKUST</i><br/>
                        [<a href="src/dnn_report.pdf" target="_blank">pdf</a>]<br/>
                        <p>We test, analyze, and compare the performance and efficiency across different deep learning models.</p>
                    </li><br/>
                    <li>
                        <b style="font-size: medium;">Press Release Classification on Mobile Application</b><br/>
                        <u>Shiu-hong Kao</u>, Dimitris Chatzopoulos<br/>
                        <i>Undergraduate Research Opportunities Program, Spring 2021, HKUST</i><br/>
                        [<a href="src/AndroidReport.pdf" target="_blank">pdf</a>]<br/>
                        <p>We develop a text-classifying AI model compact with memory-limited mobile devices.</p>
                    </li><br/>
                    <li>
                        <b style="font-size: medium;">AlignKD: A Low-cost Technique for Convolutional Shortcut Removal</b><br/>
                        <u>Shiu-hong Kao</u>, Bo-rong Lai<br/>
                        <i>Course project for COMP4471 Deep Learning in Computer Vision, HKUST</i><br/>
                        [<a href="src/AlignKD.pdf" target="_blank">pdf</a>]<br/>
                        <p>AlignKD is a cheap method to remove shortcuts in convolutional neural networks while preserving the performance.</p>
                    </li>
                </ul>-->
            </div>
        </div>
    </div>
    <div style="width:100%; background-color:rgb(249, 239, 226)">
        <div id="Experience" style="text-align: left; width:100%; max-width: 1100px; padding-bottom:40px; padding-top:40px; " class="anchor-target">
            <div style="width:90%; margin-left: 5%; margin-right: 5%;">
                <h3 style="font-size:23px; font-family: 'Lato', Verdana, Helvetica, sans-serif; text-transform: capitalize; padding-left: 20pt;text-decoration: underline;">Industrial Experience</h3>
                <table id="pub_table" style="width:100%">
                    <tr>
                        <td class="pub_left desktop">
                            <div class='one'><center><a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/" target="_blank"><img class="pub_img" src="src/msra-fig.png" /></a></center></div>
                        </td>
                        <td class="pub_right">
                            <div class='one phone'><center><a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/" target="_blank"><img class="pub_img_phone" src="src/msra-fig.png"/></a></center></div>
                            <b style="font-size: medium;">Research Intern</b><br/>
                            Media Computing Group, Microsoft Research Asia<br/>
                            <i>Jun 2024 - Dec 2024</i><br/>
                        <p>Conduct cutting-edge research and publish state-of-the-art papers in the 3D computer vision domain, contributing significantly to advancements in the research community.</p>
                        </td>
                    </tr>
                    <tr>
                        <td class="pub_left desktop">
                            <div class='one'><center><a href="https://www.astri.org/" target="_blank"><img class="pub_img" src="src/astri-fig.png" /></a></center></div>
                        </td>
                        <td class="pub_right">
                            <div class='one phone'><center><a href="https://www.astri.org/" target="_blank"><img class="pub_img_phone" src="src/astri-fig.png"/></a></center></div>
                            <b style="font-size: medium;">Software Engineer Intern</b><br/>
                            Hong Kong Applied Science and Technology Research Institute (ASTRI)<br/>
                            <i>Jun 2021 - Aug 2021</i><br/>
                            <p>Design and develop algorithms to solve problems; researching the latest technologies; tasks including image processing, OCR, data parsing, deep learning, etc. [<a href="src/InternReport.pdf" target="_blank">report</a>]</p>
                        </td>
                    </tr>
                </table>
                <h3 style="font-size:23px; font-family: 'Lato', Verdana, Helvetica, sans-serif; text-transform: capitalize; padding-left: 20pt;text-decoration: underline;">Academic Services</h3>
                <ul>
                    <li>
                        <b>Reviewer</b>: ICLR 2025
                    </li>
                    <li>
                        <b>Sub-reviewer</b>: BMVC 2023, ICLR 2024, CVPR 2024
                    </li>
                </ul>
                <h3 style="font-size:23px; font-family: 'Lato', Verdana, Helvetica, sans-serif; text-transform: capitalize; padding-left: 20pt;text-decoration: underline;">Teaching Assistant</h3>
                <ul>
                    <li>
                        COMP4471/ELEC4240 Deep Learning in Computer Vision (Spring 2025)
                    </li>
                    <li>
                        COMP4411 Computer Graphics (Spring 2024)
                    </li>
                    <li>
                        COMP2611 Computer Organization (Fall 2022, Spring 2023)
                    </li>
                </ul>
                <!--<h3 style="font-size:23px; font-family: 'Lato', Verdana, Helvetica, sans-serif; text-transform: capitalize; padding-left: 20pt;text-decoration: underline;">Talks</h3>
                <ul>
                    <li>
                        <b style="font-size: medium;">NCKU 2023 AIoT Training Camp with Special Topic on Autonomous Vehicles</b><br/>
                        Jing Ping Educational Creation Ltd., National Cheng Kung University (NCKU)<br/>
                        <i>Jul 2023</i><br/>
                    </li>
                    <li>
                        <b style="font-size: medium;">Divide and Distill: Achieving Higher Accuracy, Speed and Data-Efficiency for Knowledge Distillation</b>
                        <br/>The Hong Kong University of Science and Technology (HKUST)<br/>
                        <i>May 2023</i><br/>
                    </li>
                </ul>-->
            </div>
        </div>
    </div>
    <div style="width:100%; background-color:white">
        <div id="Awards" style="text-align: left; width:100%; max-width: 1100px; padding-top: 40px; padding-bottom: 40px;" class="anchor-target">
            <div style="width:90%; margin-left: 5%; margin-right: 5%;">
                <h3 style="font-size:23px; font-family: 'Lato', Verdana, Helvetica, sans-serif; text-transform: capitalize; padding-left: 20pt;text-decoration: underline;">Awards & Honors</h3>
                <ul>
                    <li>NUS Research Scholarship, (2026)</li>
                    <li>Stars of Tomorrow Internship Award of Excellence, Microsoft Research (2025)</li>
                    <li>Postgraduate Scholarship, HKUST (2023-25)</li>
                    <li>First Class Honors, HKUST (2023)</li>
                    <li>University Scholarship for Continuing Undergraduate Students, HKUST (2022, 2023)</li>
                    <li>Dean's List, HKUST (Fall 2019, Fall 2020, Spring 2021, Fall 2022, Spring 2023)</li>
                    <li>Dean's List (High Honors), Northwestern University (Spring 2022)</li>
                    <li>S.S. Chern Class Scholarship, HKUST (2021)</li>
                    <li>Silver Medal, Yau's Award, National Taiwan University (2018)</li>
                </ul>
            </div>
        </div>
    </div>
    </center>

    <div id="footer">
        <hr/>
        <center>
            <script type="text/javascript" src="//clustrmaps.com/globe.js?d=_ofYGB4FObWdYaSCnIB-UVg9hdUzBVm1P4rUKEjg9NU"></script>
            <small style="color: rgb(160, 160, 160)">© Shiu-hong Kao | Last updated: Feb. 2026.<br/>
        </center>
        
    </div>
    
  </body>
</html>
